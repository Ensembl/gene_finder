{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext\n",
    "\n",
    "useful links: <br>\n",
    "    https://pytorch.org/text/data.html#field<br>\n",
    "    https://www.kaggle.com/maxl28618/toxic-comments-lstm-in-pytorch-with-torchtext/notebook<br>\n",
    "    https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f<br>\n",
    "    https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "# Preliminaries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,fbeta_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#from pytorch_lightning import metrics\n",
    "#from pytorch_lightning.metrics import F1\n",
    "#from pytorch_lightning.metrics import Precision\n",
    "#from pytorch_lightning.metrics import Recall\n",
    "#from pytorch_lightning.metrics import MeanAbsoluteError\n",
    "#from pytorch_lightning.metrics import MeanSquaredError\n",
    "#from pytorch_lightning.metrics import ConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!poetry add torchtext#torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='app.log', filemode='w', format='%(asctime)s,%(msecs)d %(name)s - %(levelname)s - %(message)s',datefmt='%H:%M:%S', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for Colab\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "csv.field_size_limit(min(sys.maxsize, 2147483646))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder='/data/'\n",
    "#'/content/drive/MyDrive/gene_calling/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/panda/ensembl/ftricomi/.pyenv/versions/3.8.6/envs/gene-finder/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "10.2\n",
      "True\n",
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"{torch.__version__}\")\n",
    "print(f\"{torch.version.cuda}\")\n",
    "print(f\"{torch.backends.cudnn.enabled}\")\n",
    "print(f\"{torch.cuda.is_available()}\")\n",
    "print(f\"{device}\")\n",
    "if torch.cuda.is_available():\n",
    "        print(f\"{torch.cuda.get_device_properties(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as fh:\n",
    "        data = pickle.load(fh)\n",
    "    return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/content/drive/MyDrive/gene_calling/word_df_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_pickle(\"data/word_df_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoding</th>\n",
       "      <th>genomic_sequence</th>\n",
       "      <th>sequence_len</th>\n",
       "      <th>sequence_splitted</th>\n",
       "      <th>word_count</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>lexical density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>GGATCCGTTCGAAACAGGTTAGCCTACTATAATATAAGGATTGGAT...</td>\n",
       "      <td>11917</td>\n",
       "      <td>GGA TCC GTT CGA AAC AGG TTA GCC TAC TAT AAT AT...</td>\n",
       "      <td>3973</td>\n",
       "      <td>65</td>\n",
       "      <td>0.016360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "      <td>ATG</td>\n",
       "      <td>3</td>\n",
       "      <td>ATG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>TTTCAGTTCGCTAAGTTTTCAAAGTCCAAAGAGCGCAGACTAGCCA...</td>\n",
       "      <td>318</td>\n",
       "      <td>TTT CAG TTC GCT AAG TTT TCA AAG TCC AAA GAG CG...</td>\n",
       "      <td>106</td>\n",
       "      <td>46</td>\n",
       "      <td>0.433962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>TAA</td>\n",
       "      <td>3</td>\n",
       "      <td>TAA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>AGGCAAAGAAAAAAGGGATCCGCCTCGAATCAAAACGTTCTTTCTT...</td>\n",
       "      <td>4614</td>\n",
       "      <td>AGG CAA AGA AAA AAG GGA TCC GCC TCG AAT CAA AA...</td>\n",
       "      <td>1538</td>\n",
       "      <td>64</td>\n",
       "      <td>0.041612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encoding                                   genomic_sequence  sequence_len  \\\n",
       "0        O  GGATCCGTTCGAAACAGGTTAGCCTACTATAATATAAGGATTGGAT...         11917   \n",
       "1        S                                                ATG             3   \n",
       "2        C  TTTCAGTTCGCTAAGTTTTCAAAGTCCAAAGAGCGCAGACTAGCCA...           318   \n",
       "3        E                                                TAA             3   \n",
       "4        O  AGGCAAAGAAAAAAGGGATCCGCCTCGAATCAAAACGTTCTTTCTT...          4614   \n",
       "\n",
       "                                   sequence_splitted  word_count  vocabulary  \\\n",
       "0  GGA TCC GTT CGA AAC AGG TTA GCC TAC TAT AAT AT...        3973          65   \n",
       "1                                                ATG           1           1   \n",
       "2  TTT CAG TTC GCT AAG TTT TCA AAG TCC AAA GAG CG...         106          46   \n",
       "3                                                TAA           1           1   \n",
       "4  AGG CAA AGA AAA AAG GGA TCC GCC TCG AAT CAA AA...        1538          64   \n",
       "\n",
       "   lexical density  \n",
       "0         0.016360  \n",
       "1         1.000000  \n",
       "2         0.433962  \n",
       "3         1.000000  \n",
       "4         0.041612  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'S': 1, 'C': 2, 'E': 3, 'J': 4, 'I': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode label \n",
    "label_encoder = {l: i for i, l in enumerate(data['encoding'].unique())}\n",
    "data['encoding'] = data['encoding'].apply(lambda y: label_encoder[y])\n",
    "label_encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.155759e+06\n",
       "mean     3.150967e+02\n",
       "std      5.127760e+03\n",
       "min      1.000000e+00\n",
       "25%      1.000000e+00\n",
       "50%      3.000000e+00\n",
       "75%      4.500000e+01\n",
       "max      1.666670e+05\n",
       "Name: sequence_splitted, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=(data['sequence_splitted'].apply(lambda x: len((str(x).split())))) # counts the number of words for  max_length\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_0_data=data[data['encoding']==0]\n",
    "subset_1_data=data[data['encoding']==1]\n",
    "subset_2_data=data[data['encoding']==2]\n",
    "subset_3_data=data[data['encoding']==3]\n",
    "subset_4_data=data[data['encoding']==4]\n",
    "subset_5_data=data[data['encoding']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced dataset\n",
    "df_subset=subset_0_data[0:2000]\n",
    "df_subset=df_subset.append(subset_1_data[0:2000],ignore_index=True)\n",
    "df_subset=df_subset.append(subset_2_data[0:2000],ignore_index=True)\n",
    "df_subset=df_subset.append(subset_3_data[0:2000],ignore_index=True)\n",
    "df_subset=df_subset.append(subset_4_data[0:2000],ignore_index=True)\n",
    "df_subset=df_subset.append(subset_5_data[0:2000],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_subset['sequence_splitted'], df_subset['encoding'], test_size=0.15, random_state=1, stratify=df_subset['encoding'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1, stratify=y_train)\n",
    "\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "X_val.reset_index(inplace=True, drop=True)\n",
    "y_val.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8160, 2040, 1800)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_val),len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sequence_splitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TGC AAC ACA GAG CAC CCA TCT CTC TCC TTC AAC TC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>AAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ATG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>AAG TGC CTT TTT TTT CAA ACA TTT CTT TCA AGT CT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                  sequence_splitted\n",
       "0      0  TGC AAC ACA GAG CAC CCA TCT CTC TCC TTC AAC TC...\n",
       "1      4                                                AAG\n",
       "2      1                                                ATG\n",
       "3      4                                                TAG\n",
       "4      5  AAG TGC CTT TTT TTT CAA ACA TTT CTT TCA AGT CT..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.DataFrame()\n",
    "df_train['label']=y_train\n",
    "df_train['sequence_splitted']=X_train\n",
    "\n",
    "df_valid=pd.DataFrame()\n",
    "df_valid['label']=y_val\n",
    "df_valid['sequence_splitted']=X_val\n",
    "\n",
    "df_test=pd.DataFrame()\n",
    "df_test['label']=y_test\n",
    "df_test['sequence_splitted']=X_test\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write preprocessed data\n",
    "df_train.to_csv('data/train2.csv', index=False)\n",
    "df_valid.to_csv( 'data/valid2.csv', index=False)\n",
    "df_test.to_csv( 'data/test2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing with Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fields():\n",
    "    # Fields for encoding\n",
    "    tokenize = lambda x: x.split(' ')\n",
    "\n",
    "    text_field = Field(sequential=True, tokenize=tokenize,lower=False, include_lengths=True, batch_first=True,pad_token='O')\n",
    "    label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "\n",
    "    #same order and same column name as in the dataframe \n",
    "    fields = [ ('label', label_field),('sequence_splitted', text_field)]\n",
    "\n",
    "    return text_field, label_field, fields\n",
    "\n",
    "def load_tabular_dataset(fields):    \n",
    "    #train='train.csv', validation='valid.csv', test='test.csv',\n",
    "    train, valid, test = TabularDataset.splits(path='/content/drive/MyDrive/gene_calling/', train='train2.csv', validation='valid2.csv', test='test2.csv',\n",
    "                                                format='CSV', fields=fields, skip_header=True)\n",
    "    return train, valid, test\n",
    "\n",
    "    # Iterators\n",
    "def load_iterators(train, valid, test):   \n",
    "    \n",
    "    #each batch is of type torch.LongTensor, they are the numericalized batch\n",
    "    train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.sequence_splitted),\n",
    "                                device=device, sort=True, sort_within_batch=True)\n",
    "    valid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.sequence_splitted),\n",
    "                                device=device, sort=True, sort_within_batch=True)\n",
    "    test_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.sequence_splitted),\n",
    "                                device=device, sort=True, sort_within_batch=True)\n",
    "\n",
    "    return train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields for encoding\n",
    "tokenize = lambda x: x.split(' ')\n",
    "\n",
    "text_field = Field(sequential=True, tokenize=tokenize,lower=False, include_lengths=True, batch_first=True,pad_token='O')\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "\n",
    "#use_vocab=False have preprocessed the label to be integers\n",
    "#TorchText will dynamically pad each sequence to the longest in the batch.\n",
    "\n",
    "#same order and same column name as in the dataframe \n",
    "fields = [ ('label', label_field),('sequence_splitted', text_field)]\n",
    "\n",
    "# TabularDataset\n",
    "train, valid, test = TabularDataset.splits(path='./data/', train='train2.csv', validation='valid2.csv',\n",
    "                                            test='test2.csv',format='CSV', fields=fields, skip_header=True)\n",
    "                                        \n",
    "\n",
    "# Iterators\n",
    "#each batch is of type torch.LongTensor, they are the numericalized batch\n",
    "#BuketIterator Defines an iterator that batches examples of similar lengths together.Minimizes amount of \n",
    "#padding needed while producing freshly shuffled batches for each new epoch\n",
    "train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.sequence_splitted),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.sequence_splitted),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "test_iter = BucketIterator(test, batch_size=1, device=device, sort=False, sort_within_batch=False, repeat=False)\n",
    "\n",
    "\"\"\"\n",
    "sort_within_batch must be flagged True if you want to use PyTorch’s padded sequence class.\n",
    "repeat=None If you don’t want an epoch / iteration difference (epoch = a full traversal of your dataset, iteration = processed \n",
    "one batch from your dataset), and you want a simple while loop and a stop condition, then you should use repeat\n",
    "It means that instead of iterating for one epoch, the iterator continues infinitely until stopped (in cases where you measure performance \n",
    "after # iterations and not # epochs).\n",
    "The default will soon be changed to False (so the Uniterator will stop after one epoch).\n",
    "use this so the loop will stop when training data is traversed once\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for data in train_iter:\n",
    "    (x, x_lengths), y = data.Text, data.Description\n",
    "\"\"\"\n",
    "\n",
    "# Vocabulary\n",
    "text_field.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 64, 1800)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter),len(valid_iter),len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count \n",
    "text_field.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 1360, '4': 1360, '1': 1360, '5': 1360, '3': 1360, '2': 1360})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_field.build_vocab(train)\n",
    "label_field.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '0', '1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#enumerate labels\n",
    "label_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x2ae5a371dcd0>>,\n",
       "            {'<unk>': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding\n",
    "label_field.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Why pack sequences in pytorch explanation:\n",
    "https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## DEFINE MODEL ########################\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        #batch_size\n",
    "        self.dimension = dimension \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional) #dropout=dropout if num_layers>1\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        #dense layer output from lstm bidirectional (batch_size, 2*batch_size)\n",
    "        self.linear = nn.Linear(2*hidden_dim, 6) \n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "        #we pad the sequence for lst through embedding layer. In alternative we need to pad the sequence pad_packed_sequence\n",
    "        text_emb = self.drop(self.embedding(text))\n",
    "        #lstm_out, (ht, ct) = self.lstm(text_emb) ht=hidden ct=cell\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input)\n",
    "\n",
    "        #unpack the output from lstm [dimension, 1, 2*dimension]      \n",
    "        #output_lengths  is the length of each sentence\n",
    "        output_padded, output_lengths= pad_packed_sequence(packed_output, batch_first=True)\n",
    "       \n",
    "        #get the foward and reverse lstm outputs, concatenate both of them and pass it to the fully connected layer.\n",
    "        out_forward = output_padded[range(len(output_padded)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output_padded[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "\n",
    "        text_fea = self.drop(out_reduced)\n",
    "        \n",
    "        text_fea = self.linear(text_fea)\n",
    "        #remove size 1 [dimension,  #classes] \n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        \n",
    "        \n",
    "        #text_out=self.linear(ht[-1])\n",
    "\n",
    "        #text_out=torch.nn.LogSoftmax(text_fea)\n",
    "        return text_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "    logging.info(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    logging.info(f'Model loaded from <== {load_path}')\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "    logging.info(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    logging.info(f'Model loaded from <== {load_path}')\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,criterion ,train_loader):\n",
    "    running_loss = 0.0 \n",
    "    global_step = 0 \n",
    "    #sum_loss = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    print('start training')\n",
    "    #logging.info('start training'')\n",
    "    model.train()       \n",
    "    \n",
    "    for (label,(text, text_len)), _  in train_loader: \n",
    "       \n",
    "       if torch.cuda.is_available():\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "            text_len = text_len.to(device)\n",
    "            output = model(text, text_len)\n",
    "    \n",
    "            loss_f = nn.CrossEntropyLoss()\n",
    "            loss=loss_f(output, label.long())\n",
    "            #loss = F.cross_entropy(output, label.long())\n",
    "            #loss=criterion(output, label.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update train loss\n",
    "            running_loss += loss.item()\n",
    "            #train loss tentative 2   sum_loss/total,\n",
    "            #sum_loss += loss.item()*label.shape[0]\n",
    "            \n",
    "            #train accuracy\n",
    "            pred = torch.max(output, 1)[1]\n",
    "            correct += (pred == label).float().sum()\n",
    "            total += label.shape[0]\n",
    "           \n",
    "            #train acc\n",
    "            #batch_acc = train_accuracy(output.cpu(), label.cpu())\n",
    "            \n",
    "            # count steps\n",
    "            global_step += 1 \n",
    "            if global_step % 50 == 0: \n",
    "                print(global_step)\n",
    "                #logging.info(global_step)\n",
    "    return running_loss, global_step, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,criterion ,valid_loader):\n",
    "    valid_running_loss = 0.0    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_rmse = 0.0\n",
    "    sum_mae = 0.0\n",
    "    sum_f1=0.0\n",
    "    sum_prec=0.0\n",
    "    sum_rec=0.0\n",
    "    print('validate model')\n",
    "    logging.info('validate model')\n",
    "    #sum_loss = 0.0\n",
    "    #batch_f1=0.0\n",
    "    #batch_acc=0.0\n",
    "    model.eval()\n",
    "  \n",
    "    with torch.no_grad():                    \n",
    "        # validation loop\n",
    "        for (label, (text, text_len)), _ in valid_loader:\n",
    "          \n",
    "           if torch.cuda.is_available():\n",
    "                    label = label.to(device)\n",
    "                    text = text.to(device)\n",
    "                    text_len = text_len.to(device)\n",
    "                    output = model(text, text_len)\n",
    "                    #loss = criterion(output, labels)\n",
    "                    \n",
    "                    loss_f = nn.CrossEntropyLoss()\n",
    "                    loss=loss_f(output, label.long())\n",
    "\n",
    "                    #val loss\n",
    "                    valid_running_loss += loss.item()\n",
    "                    #val loss tentative 2\n",
    "                    #sum_loss += loss.item()*label.shape[0]\n",
    "\n",
    "                    pred = torch.max(output, 1)[1]\n",
    "                    correct += (pred == label).float().sum()\n",
    "                    total += label.shape[0]\n",
    "                    \n",
    "                    #METRICS\n",
    "\n",
    "                    #pytorch_lightning not equal with sklearn, not sure about them TODO check\n",
    "                    #batch_acc = valid_accuracy.update(output.cpu(), label.cpu())\n",
    "                    #batch_f1 = f1.update(pred.cpu(), label.cpu())\n",
    "                    #batch_precision=precision.update(output.cpu(), label.cpu())\n",
    "                    #batch_recall=recall.update(output.cpu(), label.cpu())\n",
    "                    #mean_absolute_error(output.cpu()*label.shape[0], label.long()*label.shape[0])\n",
    "                    #mean_squared_error(output, label)\n",
    "\n",
    "                    sum_rmse += np.sqrt(sklearn.metrics.mean_squared_error(label.unsqueeze(-1).cpu(),pred.cpu()))*label.shape[0]\n",
    "                    sum_mae += sklearn.metrics.mean_absolute_error(label.unsqueeze(-1).cpu(),pred.cpu())*label.shape[0]\n",
    "                    sum_f1 +=f1_score(label.unsqueeze(-1).cpu(),pred.cpu(), average='macro')*label.shape[0]\n",
    "                    sum_prec +=precision_score(label.unsqueeze(-1).cpu(),pred.cpu(), average='macro', zero_division=0)*label.shape[0]\n",
    "                    sum_rec +=recall_score(label.unsqueeze(-1).cpu(),pred.cpu(), average='macro', zero_division=0)*label.shape[0]\n",
    "                    \n",
    "\n",
    "    return valid_running_loss, correct/total, sum_rmse/total, sum_mae/total, sum_prec/total, sum_rec/total, sum_f1/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion , train_loader , valid_loader , num_epochs , file_path , best_valid_loss):\n",
    "    \n",
    "    # initialize running values\n",
    "    print('train_model')\n",
    "    logging.info('train_model')\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('train_model EPOCH '+str(epoch))        \n",
    "        logging.info('train_model EPOCH '+str(epoch))\n",
    "        train_loss,global_step, train_acc = train(model,  optimizer, criterion,train_loader)\n",
    "     \n",
    "        val_loss, val_acc, val_rmse, val_rmae, val_precision, val_recall, val_f1 = validate(model, criterion, valid_loader)\n",
    "         \n",
    "        # evaluation\n",
    "        average_train_loss = train_loss / len(train_loader) \n",
    "        average_valid_loss = val_loss / len(valid_loader)\n",
    "        train_loss_list.append(average_train_loss)\n",
    "        valid_loss_list.append(average_valid_loss)\n",
    "        global_steps_list.append(global_step)\n",
    "\n",
    "        # print progress        \n",
    "        print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader), average_train_loss, average_valid_loss))     \n",
    "        print(\" train acc %.3f, val accuracy %.3f, val rmse %.3f, val rmae %.3f, val precision %.3f, val recall %.3f, val f1 %.3f\"\n",
    "         % ( train_acc, val_acc, val_rmse, val_rmae, val_precision, val_recall, val_f1)) \n",
    "        logging.info('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader), average_train_loss, average_valid_loss))\n",
    "        logging.info(\" train acc %.3f, val accuracy %.3f, val rmse %.3f, val rmae %.3f, val precision %.3f, val recall %.3f, val f1 %.3f\"\n",
    "         % ( train_acc, val_acc, val_rmse, val_rmae, val_precision, val_recall, val_f1))\n",
    "        #pytorch lightning\n",
    "        #print(\"train acc %.3f, val accuracy %.3f\" % (train_accuracy.compute(), valid_accuracy.compute()))  \n",
    "        #print(\"f1 %.3f, precision %.3f, recall %.3f, mean_absolute_error %.3f\" % (f1.compute(), precision.compute(),recall.compute(),val_rmae))      \n",
    "\n",
    "    \n",
    "        # resetting running values\n",
    "        train_loss = 0.0                \n",
    "        val_loss = 0.0        \n",
    "        global_step = 0.0    \n",
    "        val_acc = 0.0 \n",
    "        val_rmse = 0.0 \n",
    "        val_rmae = 0.0  \n",
    "        val_precision = 0.0  \n",
    "        val_recall = 0.0  \n",
    "        val_f1 = 0.0  \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        \n",
    "        # checkpoint\n",
    "        if best_valid_loss > average_valid_loss:\n",
    "            best_valid_loss = average_valid_loss\n",
    "            print(f'Best validation loss!! {best_valid_loss}')\n",
    "            logging.info(f'Best validation loss!! {best_valid_loss}')\n",
    "            save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "            save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "        save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "        print('Finished Training!')\n",
    "        logging.info('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def evaluate(model, test_loader,  threshold=0.5):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    sum_rmse = 0.0\n",
    "    sum_mae = 0.0\n",
    "    sum_f1=0.0\n",
    "    sum_prec=0.0\n",
    "    sum_rec=0.0\n",
    "    sum_acc=0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    confusion_matrix = torch.zeros(6,6)\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for (labels, (text, text_len)), _ in test_loader:  \n",
    "          \n",
    "          if torch.cuda.is_available():         \n",
    "            labels = labels.to(device)\n",
    "            text = text.to(device)\n",
    "            text_len = text_len.to(device)\n",
    "            output = model(text, text_len)\n",
    "\n",
    "            #output = (output > threshold).int()\n",
    "            #y_pred.extend(output.tolist())\n",
    "            #y_true.extend(labels.tolist())\n",
    "            #y_pred=torch.max(output.data, 1)\n",
    "            #print(y_pred)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            #preds = torch.max(output, 1)[1]\n",
    "\n",
    "            correct += (preds == labels).float().sum()\n",
    "            total += labels.shape[0]\n",
    "\n",
    "            sum_rmse += sklearn.metrics.mean_squared_error(labels.unsqueeze(-1).cpu(),preds.cpu(),squared=False)*labels.shape[0]\n",
    "            sum_mae += sklearn.metrics.mean_absolute_error(labels.unsqueeze(-1).cpu(),preds.cpu())*labels.shape[0]\n",
    "            sum_f1 +=f1_score(labels.unsqueeze(-1).cpu(),preds.cpu(), average='macro')*labels.shape[0]\n",
    "            sum_prec +=precision_score(labels.unsqueeze(-1).cpu(),preds.cpu(), average='macro', zero_division=0)*labels.shape[0]\n",
    "            sum_rec +=recall_score(labels.unsqueeze(-1).cpu(),preds.cpu(), average='macro', zero_division=0)*labels.shape[0]\n",
    "       \n",
    "    print(confusion_matrix)\n",
    "    print('Accuracy', correct/total)\n",
    "    print('Precision', sum_prec/total)\n",
    "    print('Recall', sum_rec/total)\n",
    "    print('F1', sum_f1/total)\n",
    "    print('RMSE', sum_rmse/total)\n",
    "    print('MAE', sum_mae/total)\n",
    "    logging.info(confusion_matrix)\n",
    "    logging.info(f'Accuracy {correct/total}')\n",
    "    logging.info(f'Precision {sum_prec/total}')\n",
    "    logging.info(f'Recall {sum_rec/total}')\n",
    "    logging.info(f'F1 {sum_f1/total}')\n",
    "    logging.info(f'RMSE {sum_rmse/total}')\n",
    "    logging.info(f'MAE {sum_mae/total}')\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # print version and environment information\n",
    "    print(f\"{torch.__version__}\")\n",
    "    print(f\"{torch.version.cuda}\")\n",
    "    print(f\"{torch.backends.cudnn.enabled}\")\n",
    "    print(f\"{torch.cuda.is_available()}\")\n",
    "    print(f\"{device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{torch.cuda.get_device_properties(device)}\")\n",
    "       \n",
    "    \n",
    "    destination_folder='data/'\n",
    "    \n",
    "    text_field, label_field, fields = load_fields()\n",
    "\n",
    "    train, valid, test=load_tabular_dataset(fields)\n",
    "\n",
    "    train_iter, valid_iter, test_iter= load_iterators(train, valid, test)\n",
    "\n",
    "    # Vocabulary\n",
    "    text_field.build_vocab(train)\n",
    "    label_field.build_vocab(train)\n",
    "\n",
    "    logging.info(label_field.vocab.freqs)\n",
    "\n",
    "    #Define model\n",
    "    model = LSTM(dimension=32, vocab_size=len(text_field.vocab), embedding_dim=150, hidden_dim=32, output_dim=6, n_layers=2, \n",
    "                 bidirectional=True, dropout=0.6, pad_idx=text_field.vocab.stoi[text_field.pad_token]).to(device)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    train_model(model,optimizer,criterion = nn.CrossEntropyLoss(),train_loader = train_iter,valid_loader = valid_iter,\n",
    "          num_epochs = 30,file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")) \n",
    "    logging.info(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print()\n",
    "        print(\"Interrupted with CTRL-C, exiting...\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SET MODEL PARAMETERS ########################\n",
    "#n_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together \n",
    "#to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final \n",
    "        \n",
    "#embedding_dim 100-300 even with a bigger vocabulary  \n",
    "\n",
    "#dimension=batch size \n",
    "########################################################\n",
    "\n",
    "model = LSTM(dimension=32, vocab_size=len(text_field.vocab), embedding_dim=120, hidden_dim=32, \n",
    "             output_dim=6, n_layers=1, \n",
    "                 bidirectional=True, dropout=0.3, pad_idx=text_field.vocab.stoi[text_field.pad_token]).to(device)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,134 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_lightening metrics\n",
    "#train_accuracy = metrics.Accuracy()\n",
    "#valid_accuracy = metrics.Accuracy()\n",
    "#f1 = F1(num_classes=6,compute_on_step=True,average='macro')#’macro’ computes metric for each class and uniformly averages them\n",
    "#precision = Precision(num_classes=6,average='macro')\n",
    "#recall = Recall(num_classes=6,average='macro')\n",
    "#mean_absolute_error = MeanAbsoluteError()\n",
    "#mean_squared_error = MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(model,optimizer,criterion = nn.CrossEntropyLoss(),train_loader = train_iter,valid_loader = valid_iter,\n",
    "          num_epochs = 30,file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot training and validation losses\n",
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot( train_loss_list, label='Train')\n",
    "plt.plot( valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO load model and continue training\n",
    "best_model = model.to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model, optimizer)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(best_model,optimizer,criterion = nn.CrossEntropyLoss(),train_loader = train_iter,valid_loader = valid_iter,\n",
    "          num_epochs = 10,file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchtext.data import Iterator\n",
    "#test_iter = Iterator(test, batch_size=1, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model, optimizer)\n",
    "confusion_matrix=evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix, annot=True, ax = ax, cmap='Blues')\n",
    "\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix: percentage of  data represented in each quadrant\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix/confusion_matrix.sum(), annot=True,fmt='.2%',ax = ax, cmap='Blues')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted label\n",
    "label_field.vocab.itos[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
