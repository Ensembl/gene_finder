{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add ray #1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add ray[tune] \n",
    "#Using version ^1.1.0 for ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# Libraries\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "# Preliminaries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,fbeta_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder='/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/panda/ensembl/ftricomi/.pyenv/versions/3.8.6/envs/gene-finder/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## DEFINE MODEL ######################## \n",
    "class LSTM1(nn.Module):\n",
    "     \n",
    "    def __init__(self, num_embeddings=10, embedding_dim=110, hidden_dim=32,  n_layers=1, dropout=0.2,padding_idx='0'):\n",
    "        #super().__init__()\n",
    "        super(LSTM1, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx = padding_idx)\n",
    "        #batch_size\n",
    "        self.dimension = 32\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)#dropout=dropout if num_layers>1\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        #dense layer output from lstm bidirectional (batch_size, 2*batch_size)\n",
    "        self.linear = nn.Linear(2*hidden_dim, 6) \n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "        #we pad the sequence for lst through embedding layer. In alternative we need to pad the sequence pad_packed_sequence\n",
    "        text_emb = self.drop(self.embedding(text))\n",
    "        #lstm_out, (ht, ct) = self.lstm(text_emb)\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input)\n",
    "\n",
    "        #unpack the output from lstm [dimension, 1, 2*dimension]      \n",
    "        #output_lengths  is the length of each sentence\n",
    "        output_padded, output_lengths= pad_packed_sequence(packed_output, batch_first=True)\n",
    "       \n",
    "        #get the foward and reverse lstm outputs, concatenate both of them and pass it to the fully connected layer.\n",
    "        out_forward = output_padded[range(len(output_padded)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output_padded[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "\n",
    "        text_fea = self.drop(out_reduced)\n",
    "        \n",
    "        text_fea = self.linear(text_fea)\n",
    "        #remove size 1 [dimension,  #classes] \n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        \n",
    "        \n",
    "        #text_out=self.linear(ht[-1])\n",
    "\n",
    "        #text_out=torch.nn.LogSoftmax(text_fea)\n",
    "        return text_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,criterion ,valid_loader,device):\n",
    "    valid_running_loss = 0.0    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print('validate model')\n",
    "    model.eval()\n",
    "  \n",
    "    with torch.no_grad():                    \n",
    "        # validation loop\n",
    "        for (label, (text, text_len)), _ in valid_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                    label = label.to(device)\n",
    "                    text = text.to(device)\n",
    "                    text_len = text_len.to(device)\n",
    "                    output = model(text, text_len)\n",
    "                    \n",
    "                    loss_f = nn.CrossEntropyLoss()\n",
    "                    loss=loss_f(output, label.long())\n",
    "\n",
    "                    #val loss\n",
    "                    valid_running_loss += loss.item()\n",
    "                    \n",
    "                    pred = torch.max(output, 1)[1]\n",
    "                    correct += (pred == label).float().sum()\n",
    "                    total += label.shape[0]\n",
    "\n",
    "    return valid_running_loss,correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune run in parallel so I need to load dataset and prepare the fields inside the same function\n",
    "def train_model_tuning(config, checkpoint_dir=None, data_dir=None):\n",
    "    \n",
    "    # initialize running values\n",
    "    print('train_model')\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    file_path = destination_folder,\n",
    "    best_valid_loss = float(\"Inf\")\n",
    "\n",
    "    # Fields for encoding\n",
    "    tokenize = lambda x: x.split(' ')\n",
    "    text_field = Field(sequential=True, tokenize=tokenize,lower=False, include_lengths=True, batch_first=True,pad_token='O')\n",
    "    label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "\n",
    "    fields = [ ('label', label_field),('sequence_splitted', text_field)]\n",
    "    #for Colab\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    \n",
    "    # TabularDataset                              \n",
    "    train, valid, test = TabularDataset.splits(path='/content/drive/MyDrive/gene_calling/', train='train2.csv', validation='valid2.csv', test='test2.csv',\n",
    "                                                format='CSV', fields=fields, skip_header=True)\n",
    "    # Vocabulary\n",
    "    text_field.build_vocab(train)\n",
    "\n",
    "    # Iterators\n",
    "    train_loader = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.sequence_splitted),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "    valid_loader = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.sequence_splitted),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "\n",
    "\n",
    "    model = LSTM1(num_embeddings= len(text_field.vocab), embedding_dim=config[\"embedding_dim\"], hidden_dim=config[\"hidden_dim\"], n_layers=config[\"n_layers\"], dropout=config[\"dropout\"],padding_idx = text_field.vocab.stoi[text_field.pad_token]).to(device)\n",
    "    #model = LSTM(config['embedding_dim'], config[\"hidden_dim\"], config[\"n_layers\"], config[\"dropout\"] ).to(device)\n",
    "    \n",
    "\n",
    "    #if torch.cuda.is_available():\n",
    "    #    device = \"cuda:0\"\n",
    "    #    if torch.cuda.device_count() > 1:\n",
    "    #        model = nn.DataParallel(model)\n",
    "    #model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        print('sono nel checkpoint')\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print('EPOCH', epoch)\n",
    "        \n",
    "        #TRAINING\n",
    "        #train_loss,global_step,train_acc = train(model,  optimizer, criterion,train_loader)\n",
    "        running_loss = 0.0 \n",
    "        global_step = 0 \n",
    "        total = 0.0\n",
    "        correct = 0.0\n",
    "        print('start training')\n",
    "\n",
    "        model.train()   \n",
    "    \n",
    "        for (label,(text, text_len)), _  in train_loader: \n",
    "          \n",
    "          if torch.cuda.is_available():\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "            text_len = text_len.to(device)\n",
    "            output = model(text, text_len)\n",
    "    \n",
    "            loss_f = nn.CrossEntropyLoss()\n",
    "            loss=loss_f(output, label.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update train loss\n",
    "            running_loss += loss.item()\n",
    "            #train accuracy\n",
    "            pred = torch.max(output, 1)[1]\n",
    "            correct += (pred == label).float().sum()\n",
    "            total += label.shape[0]\n",
    "            \n",
    "            # evaluate it only for few steps\n",
    "            global_step += 1 \n",
    "        \n",
    "        train_loss=running_loss\n",
    "        \n",
    "        train_acc=correct/total\n",
    "\n",
    "        #VALIDATION\n",
    "        val_loss, val_acc = validate(model, criterion, valid_loader, device)\n",
    "         \n",
    "        # evaluation\n",
    "        average_train_loss = train_loss / len(train_loader) \n",
    "        average_valid_loss = val_loss / len(valid_loader)\n",
    "        train_loss_list.append(average_train_loss)\n",
    "        valid_loss_list.append(average_valid_loss)\n",
    "        global_steps_list.append(global_step)\n",
    "\n",
    "        \n",
    "        \n",
    "        # checkpoint\n",
    "        if best_valid_loss > average_valid_loss:\n",
    "            best_valid_loss = average_valid_loss\n",
    "            print(f'Best validation loss!! {best_valid_loss}')\n",
    "            \n",
    "        \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "        print('TUNE REPORT')\n",
    "        tune.report(loss=average_valid_loss, accuracy=val_acc)\n",
    "\n",
    "        # resetting running values\n",
    "        train_loss = 0.0                \n",
    "        val_loss = 0.0\n",
    "        global_step = 0.0 \n",
    "        val_acc = 0.0 \n",
    "       \n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 trials (each trial is one instance of a Trainable). Tune runs\n",
    "# in parallel and automatically determines concurrency.\n",
    "data_dir = os.path.abspath(\"./content/drive/MyDrive/gene_calling\")\n",
    "\n",
    "config = {\n",
    "        \"embedding_dim\" : tune.grid_search([100,120,110]),#150\n",
    "        \"hidden_dim\": tune.grid_search([32]),#32\n",
    "        \"n_layers\": tune.grid_search([1,2]),\n",
    "        \"num_epochs\": tune.grid_search([15]),\n",
    "        #\"lr\": tune.loguniform(1e-3, 1e-2),  choice  0.5e-3,1e-3, 0.5e-2, 1e-2\n",
    "        \"lr\": tune.grid_search([1e-3, 0.5e-2, 0.5e-3]),\n",
    "        \"dropout\" : tune.grid_search([0.3, 0.4])\n",
    "    }    \n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=15,\n",
    "        grace_period=1,\n",
    "        reduction_factor=4)\n",
    "reporter = CLIReporter(\n",
    "         #parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "result = tune.run(\n",
    "        partial(train_model_tuning, data_dir=destination_folder),\n",
    "        resources_per_trial={ \"cpu\": 2,\"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=1,#-1 infinite\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "#choice randomly choose a value\n",
    "#grid_search try each combination of parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "#ray.init(log_to_driver=False, dashboard_port=8265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/panda/ensembl/ftricomi/.pyenv/versions/3.8.6/envs/gene-finder/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fields for encoding\n",
    "tokenize = lambda x: x.split(' ')\n",
    "text_field = Field(sequential=True, tokenize=tokenize,lower=False, include_lengths=True, batch_first=True,pad_token='O')\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "\n",
    "fields = [ ('label', label_field),('sequence_splitted', text_field)]\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "    # TabularDataset\n",
    "    #csv.field_size_limit=2147483646                               \n",
    "train, valid, test = TabularDataset.splits(path='./data/', train='train2.csv', validation='valid2.csv', test='test2.csv',\n",
    "                                                format='CSV', fields=fields, skip_header=True)\n",
    "    # Vocabulary\n",
    "text_field.build_vocab(train)\n",
    "test_iter = BucketIterator(test, batch_size=1, device=device, sort=False, sort_within_batch=False, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trained_model = LSTM1(len(text_field.vocab),best_trial.config[\"embedding_dim\"], \n",
    "                           best_trial.config[\"hidden_dim\"],\n",
    "                           best_trial.config[\"n_layers\"], best_trial.config[\"dropout\"],\n",
    "                           text_field.vocab.stoi[text_field.pad_token]).to(device)\n",
    "  \n",
    "\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "best_trained_model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir ~/ray_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 trials (each trial is one instance of a Trainable). Tune runs\n",
    "# in parallel and automatically determines concurrency.\n",
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    data_dir = os.path.abspath(\"./content/drive/MyDrive/gene_calling\")\n",
    "    \n",
    "    config = {\n",
    "        \"embedding_dim\" : tune.choice([100]),#150\n",
    "        \"hidden_dim\": tune.choice([  16]),#32\n",
    "        \"n_layers\": tune.choice([1,2]),\n",
    "        \"num_epochs\": tune.grid_search([ 5,10]),\n",
    "        \"lr\": tune.choice([1e-3, 1e-1]),\n",
    "        \"dropout\" : tune.choice([0.3, 0.5])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=10,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "         parameter_columns=[\"embedding_dim\", \"hidden_dim\", \"n_layers\", \"num_epochs\", \"lr\", \"dropout\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_model_tuning, data_dir=destination_folder),\n",
    "        resources_per_trial={ \"cpu\": 2,\"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        #scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = model(best_trial.config[\"embedding_dim\"], best_trial.config[\"hidden_dim\"],\n",
    "                               best_trial.config[\"emben_layersdding_dim\"], best_trial.config[\"num_epochs\"],\n",
    "                               best_trial.config[\"lr\"],best_trial.config[\"dropout\"]).to(device)\n",
    "  \n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
